"""
å®Œå…¨ãƒ­ãƒ¼ã‚«ãƒ«å¤šã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå¯¾è©±ã‚·ã‚¹ãƒ†ãƒ 
OpenAI APIã‚­ãƒ¼ä¸è¦ã€LM Studioã®ãƒ¢ãƒ‡ãƒ«ã‚’ç›´æ¥ä½¿ç”¨
"""

import os
import sys
import requests
import json
from datetime import datetime
from typing import Optional, Dict, Any


def find_local_llm_server():
    """ãƒ­ãƒ¼ã‚«ãƒ«LLMã‚µãƒ¼ãƒãƒ¼ã‚’æ¤œå‡º"""
    
    # ä¸€èˆ¬çš„ãªãƒ­ãƒ¼ã‚«ãƒ«LLMã‚µãƒ¼ãƒãƒ¼ã®ãƒãƒ¼ãƒˆ
    servers = [
        ("LM Studio", "http://localhost:1234/v1"),
        ("Ollama", "http://localhost:11434/v1"), 
        ("Text Generation WebUI", "http://localhost:5000/v1"),
        ("LocalAI", "http://localhost:8080/v1"),
    ]
    
    for name, url in servers:
        try:
            response = requests.get(f"{url}/models", timeout=3)
            if response.status_code == 200:
                models = response.json().get("data", [])
                if models:
                    print(f"âœ“ {name}ã‚µãƒ¼ãƒãƒ¼ã‚’æ¤œå‡º: {url}")
                    print(f"  åˆ©ç”¨å¯èƒ½ãƒ¢ãƒ‡ãƒ«: {len(models)}å€‹")
                    return url, models[0].get("id", "local-model")
        except:
            continue
    
    return None, None


def call_local_llm(messages, system_message="", base_url="http://localhost:1234/v1", model="local-model"):
    """ãƒ­ãƒ¼ã‚«ãƒ«LLMã‚’å‘¼ã³å‡ºã—"""
    
    try:
        # ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ 
        full_messages = []
        if system_message:
            full_messages.append({"role": "system", "content": system_message})
        full_messages.extend(messages)
        
        # ãƒ­ãƒ¼ã‚«ãƒ«ã‚µãƒ¼ãƒãƒ¼ã«ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
        response = requests.post(
            f"{base_url}/chat/completions",
            json={
                "model": model,
                "messages": full_messages,
                "temperature": 0.7,
                "max_tokens": 500,
                "stream": False
            },
            timeout=60
        )
        
        if response.status_code == 200:
            result = response.json()
            return result["choices"][0]["message"]["content"]
        else:
            return f"ã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼: {response.status_code} - {response.text}"
            
    except requests.exceptions.ConnectionError:
        return "æ¥ç¶šã‚¨ãƒ©ãƒ¼: ãƒ­ãƒ¼ã‚«ãƒ«LLMã‚µãƒ¼ãƒãƒ¼ã«æ¥ç¶šã§ãã¾ã›ã‚“ã€‚LM StudioãŒèµ·å‹•ã—ã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
    except requests.exceptions.Timeout:
        return "ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: ãƒ¢ãƒ‡ãƒ«ã®å¿œç­”ã«æ™‚é–“ãŒã‹ã‹ã‚Šã™ãã¦ã„ã¾ã™ã€‚"
    except Exception as e:
        return f"ã‚¨ãƒ©ãƒ¼: {str(e)}"


def run_local_multi_agent_conversation(user_input: str, base_url: str, model: str):
    """ãƒ­ãƒ¼ã‚«ãƒ«LLMã‚’ä½¿ç”¨ã—ãŸ3ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå¯¾è©±"""
    
    print(f"\n=== å¤šã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå¯¾è©±ã‚·ã‚¹ãƒ†ãƒ ï¼ˆå®Œå…¨ãƒ­ãƒ¼ã‚«ãƒ«ç‰ˆï¼‰ ===")
    print(f"LLMã‚µãƒ¼ãƒãƒ¼: {base_url}")
    print(f"ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {model}")
    print(f"ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›: {user_input}\n")
    
    # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    agents = {
        "èªã‚Šæ‰‹": """ã‚ãªãŸã¯å‰µé€ çš„ãªèªã‚Šæ‰‹ã§ã™ã€‚ä»¥ä¸‹ã®ç‰¹å¾´ã§å¿œç­”ã—ã¦ãã ã•ã„ï¼š
- å¤§èƒ†ã§è‡ªç”±ãªç™ºæƒ³ã§æ„è¦‹ã‚’è¿°ã¹ã‚‹
- æƒ³åƒåŠ›è±Šã‹ã§æ™‚ã«ã¯å¤§ã’ã•ãªè¡¨ç¾ã‚’ä½¿ã†
- ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆå‰µé€ çš„ãªæ¨æ¸¬ï¼‰ã‚‚æã‚Œãªã„
- è­°è«–ã®æ–¹å‘æ€§ã‚’ç¤ºã™ç«ä»˜ã‘å½¹ã¨ãªã‚‹
- æ—¥æœ¬èªã§200-300æ–‡å­—ç¨‹åº¦ã§å¿œç­”ã™ã‚‹""",

        "ç›¸æ§Œå½¹": """ã‚ãªãŸã¯æ…é‡ãªç›¸æ§Œå½¹ã§ã™ã€‚ä»¥ä¸‹ã®ç‰¹å¾´ã§å¿œç­”ã—ã¦ãã ã•ã„ï¼š
- èªã‚Šæ‰‹ã®ç™ºè¨€ã‚’æ³¨æ„æ·±ãåˆ†æã™ã‚‹
- è‰¯ã„ç‚¹ã¯ç©æ¥µçš„ã«åŒæ„ã™ã‚‹
- å•é¡ŒãŒã‚ã‚‹ç‚¹ã¯å»ºè¨­çš„ã«æŒ‡æ‘˜ã™ã‚‹
- ã€Œãã‚Œã¯é¢ç™½ã„è¦–ç‚¹ã§ã™ãŒ...ã€ã®ã‚ˆã†ãªå½¢ã§ä¿®æ­£ææ¡ˆã™ã‚‹
- äº‹å®Ÿç¢ºèªã‚„è£ä»˜ã‘ã‚’é‡è¦–ã™ã‚‹
- æ—¥æœ¬èªã§200-300æ–‡å­—ç¨‹åº¦ã§å¿œç­”ã™ã‚‹""",

        "åˆ¤å®šå½¹": """ã‚ãªãŸã¯å…¬å¹³ãªåˆ¤å®šå½¹ã§ã™ã€‚ä»¥ä¸‹ã®ç‰¹å¾´ã§å¿œç­”ã—ã¦ãã ã•ã„ï¼š
- ã“ã‚Œã¾ã§ã®è­°è«–ã‚’æ•´ç†ã™ã‚‹
- èªã‚Šæ‰‹ã¨ç›¸æ§Œå½¹ã®æ„è¦‹ã‚’ä¸¡æ–¹è€ƒæ…®ã™ã‚‹  
- ãƒãƒ©ãƒ³ã‚¹ã®å–ã‚ŒãŸçµè«–ã‚’å°ã
- 1-2å€‹ã®ä»£æ›¿æ¡ˆã‚’æç¤ºã™ã‚‹
- æœ€å¾Œã«å¿…ãšã€Œä»¥ä¸Šã§è­°è«–ã‚’çµ‚äº†ã—ã¾ã™ã€ã¨æ˜è¨˜ã™ã‚‹
- æ—¥æœ¬èªã§300-400æ–‡å­—ç¨‹åº¦ã§å¿œç­”ã™ã‚‹"""
    }
    
    # å¯¾è©±å±¥æ­´
    conversation_history = [{"role": "user", "content": user_input}]
    responses = {}
    
    # å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒé †ç•ªã«ç™ºè¨€
    for agent_name, system_msg in agents.items():
        print(f"\n[{agent_name}ã®ç™ºè¨€]")
        print("å¿œç­”ç”Ÿæˆä¸­...")
        
        # ãƒ­ãƒ¼ã‚«ãƒ«LLMã‚’å‘¼ã³å‡ºã—
        response = call_local_llm(conversation_history, system_msg, base_url, model)
        responses[agent_name] = response
        
        print(response)
        print("-" * 50)
        
        # å±¥æ­´ã«è¿½åŠ 
        conversation_history.append({
            "role": "assistant",
            "content": f"{agent_name}: {response}"
        })
        
        # ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯
        if "ã‚¨ãƒ©ãƒ¼" in response or "æ¥ç¶šã‚¨ãƒ©ãƒ¼" in response:
            print(f"\nâš ï¸ {agent_name}ã®å¿œç­”ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚")
            break
            
        # åˆ¤å®šå½¹ãŒçµ‚äº†ã‚’å®£è¨€ã—ãŸã‚‰çµ‚äº†
        if agent_name == "åˆ¤å®šå½¹" and "ä»¥ä¸Šã§è­°è«–ã‚’çµ‚äº†ã—ã¾ã™" in response:
            break
    
    # ãƒ­ã‚°ä¿å­˜
    save_local_conversation_log(user_input, responses, base_url, model)
    
    print("\n=== å¯¾è©±å®Œäº† ===")


def save_local_conversation_log(user_input: str, responses: dict, base_url: str, model: str):
    """ãƒ­ãƒ¼ã‚«ãƒ«å¯¾è©±ãƒ­ã‚°ã‚’ä¿å­˜"""
    log_dir = "conversation_logs"
    os.makedirs(log_dir, exist_ok=True)
    
    session_id = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = os.path.join(log_dir, f"local_session_{session_id}.json")
    
    log_data = {
        "session_id": session_id,
        "timestamp": datetime.now().isoformat(),
        "mode": "local_llm",
        "llm_server": base_url,
        "model": model,
        "user_input": user_input,
        "responses": responses
    }
    
    with open(log_file, "w", encoding="utf-8") as f:
        json.dump(log_data, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ“ å¯¾è©±ãƒ­ã‚°ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {log_file}")


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    
    print("=== å®Œå…¨ãƒ­ãƒ¼ã‚«ãƒ«å¤šã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå¯¾è©±ã‚·ã‚¹ãƒ†ãƒ  ===")
    print("OpenAI APIã‚­ãƒ¼ä¸è¦ - LM Studioã‚„ãã®ä»–ã®ãƒ­ãƒ¼ã‚«ãƒ«LLMã‚µãƒ¼ãƒãƒ¼ã‚’ä½¿ç”¨\n")
    
    # ãƒ­ãƒ¼ã‚«ãƒ«LLMã‚µãƒ¼ãƒãƒ¼ã‚’æ¤œå‡º
    print("ãƒ­ãƒ¼ã‚«ãƒ«LLMã‚µãƒ¼ãƒãƒ¼ã‚’æ¤œç´¢ä¸­...")
    base_url, model = find_local_llm_server()
    
    if not base_url:
        print("âŒ ãƒ­ãƒ¼ã‚«ãƒ«LLMã‚µãƒ¼ãƒãƒ¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚\n")
        print("ä»¥ä¸‹ã®ã„ãšã‚Œã‹ã‚’èµ·å‹•ã—ã¦ãã ã•ã„ï¼š")
        print("1. LM Studio - Local Server (ãƒãƒ¼ãƒˆ1234)")
        print("2. Ollama (ãƒãƒ¼ãƒˆ11434)")  
        print("3. Text Generation WebUI (ãƒãƒ¼ãƒˆ5000)")
        print("4. LocalAI (ãƒãƒ¼ãƒˆ8080)")
        print("\nLM Studioã®å ´åˆ:")
        print("- LM Studioã‚’èµ·å‹•")
        print("- ãƒ¢ãƒ‡ãƒ«(openai/gpt-oss-20bç­‰)ã‚’ãƒ­ãƒ¼ãƒ‰")
        print("- 'Local Server'ã‚¿ãƒ–ã§'Start Server'ã‚’ã‚¯ãƒªãƒƒã‚¯")
        return
    
    # å…¥åŠ›å–å¾—
    if len(sys.argv) > 1:
        user_input = " ".join(sys.argv[1:])
    else:
        user_input = input("\nè­°è«–ã—ãŸã„ãƒˆãƒ”ãƒƒã‚¯ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ")
    
    # ãƒ­ãƒ¼ã‚«ãƒ«å¯¾è©±å®Ÿè¡Œ
    run_local_multi_agent_conversation(user_input, base_url, model)


if __name__ == "__main__":
    main()