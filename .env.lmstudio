# LM Studio Configuration
# LM Studioを使用する場合の設定

# LM StudioのローカルサーバーURL（デフォルトポート: 1234）
API_BASE_URL=http://localhost:1234/v1

# LM Studioで読み込んでいるモデル名
# 例: TheBloke/Mistral-7B-Instruct-v0.2-GGUF
# 例: TheBloke/Llama-2-13B-chat-GGUF
# 例: mistralai/Mistral-7B-Instruct-v0.2
MODEL_NAME=TheBloke/Mistral-7B-Instruct-v0.2-GGUF

# OpenAI APIキーは不要（ダミー値でOK）
OPENAI_API_KEY=lm-studio

# 温度設定（0.0-1.0、高いほど創造的）
TEMPERATURE=0.7